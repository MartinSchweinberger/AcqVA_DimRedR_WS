---
title: "Introduction to Dimension Reduction Methods with R - AcqVA Aurora workshop"
author: "Martin Schweinberger"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:
  bookdown::html_document2: 
bibliography: bibliography.bib
link-citations: yes
---


```{r uq1, echo=F, fig.cap="", message=FALSE, warning=FALSE, out.width='100%'}
knitr::include_graphics("https://slcladal.github.io/images/acqvalab.png")
```


# Introduction{-}

This tutorial introduces dimension reduction methods with R with the aim of showcasing how these methods work, how to prepare data and how to implement selected dimension reduction methods (Principal Component Analysis, Factor Analysis, and Multidimensional Scaling). 

```{r diff, echo=FALSE, out.width= "15%", out.extra='style="float:right; padding:10px"'}
knitr::include_graphics("https://slcladal.github.io/images/gy_chili.jpg")
```

This Workshop is aimed at beginners and intermediate users of R. The aim is not to provide a fully-fledged guide but rather to show and exemplify some common dimension reduction methods with R.


<div class="warning" style='padding:0.1em; background-color:#f2f2f2; color:#51247a'>
<span>
<p style='margin-top:1em; text-align:center'>
The entire R Notebook for the tutorial can be downloaded [**here**](https://github.com/MartinSchweinberger/AcqVA_DimRedR_WS/acqvadimredrws.Rmd).  If you want to render the R Notebook on your machine, i.e. knitting the document to html or a pdf, you need to make sure that you have R and RStudio installed and you also need to download the [**bibliography file**](https://github.com/MartinSchweinberger/AcqVA_DimRedR_WS/bibliography.bib) and store it in the same folder where you store the Rproj file. <br><br>
**[Here](https://colab.research.google.com/drive/16VNNnRXAC6CFU9oBwLlQsoG_Xpub-CRl?usp=sharing)** is a **link to an interactive version of this tutorial on Binder**. The interactive tutorial is based on a Jupyter notebook of this tutorial. This interactive Jupyter notebook allows you to execute code yourself and - if you copy the Jupyter notebook - you can also change and edit the notebook, e.g. you can change code and upload your own data.<br></p>
<p style='margin-left:1em;'>
</p></span>
</div>

<br>



## What to do before the workshop{-}

To get the most out of this workshop, you will need to have some (basic) R skills and (basic) knowledge of how to work with R, RStudio, R Projects, and R Notebooks. If you have no or little experience with this or if you need to refresh your skills, please carefully read (or optimally go through) these tutorials:

* [Getting started with R](https://slcladal.github.io/intror.html)

* [Handling tables in R](https://slcladal.github.io/table.html)

Before attending the workshop, you need to install R and RStudio. If you have already installed R, you need to update R and existing packages. You can update R and existing packages using the code chunk below (you need to un-comment the commands from them to becoe active).

```{r update, eval = F, message=F, warning=F}
# update R
#install.packages("installr")
#library(installr)
#updateR()
```


You can update R and install these packages by clicking on `packages` on the top of the lower right pane in RStudio or you can execute the following code in the lower left, `Console` pane of RStudio. 

```{r install, eval = F, message=F, warning=F}
# install required packages
install.packages(c(
  
  # packages for data processing
  "dplyr", "here", "flextable", "stringr", "tidyr",
  
  # packages for principal component analysis
  "MASS", "factoextra", 
  
  # packages for visualizing and reporting results
  "ggplot2", "report"), 
  
  # and we also want to have all dependencies installed
  dependencies = T)
```


**It is really important that you have knowledge of R and RStudio and that you have installed the packages before the workshop so that we do not have to deal with technical issues too much.**

After installing these packages, we fetch them from the library to activate the packages.

```{r activate, message=F, warning=F}
library(dplyr)
library(here)
library(flextable)
library(stringr)
library(tidyr)
library(MASS)
library(factoextra)
library(ggplot2)
library(report)
```

You can find a more detailed version of the content of this workshop in these two LADAL tutorials

* [Introduction to Data Visualization in R](https://slcladal.github.io/introviz.html)

* [Data Visualization with R](https://slcladal.github.io/dviz.html)

You can follow this workshop in different ways - you can sit back and watch it like a lecture or take a more active role - that said, the intention for this workshop is clearly to be practical so that I show something and then you do it on you computer and we have exercises where you can try out what you have just learned.

So, in essence, there are the following three options for following this workshop:

1. You can sit back and enjoy and focus on what you see and then go back home and try it by yourself later.

2. You can follow this workshop in RStudio and execute code, see what it does, understand it, and adapt it. This requires some skills in R and RStudio - although I have trued to keep things simple.

3. You can click on [this link]() which takes you to a Jupyter Notebook on Google Colab (and you are then ready to go - you only  have to install the packages which takes a couple of minutes). You can then copy that Notebook and save it in your own Google Drive and then execute code, modify it, and understand it. This does require less skills and will be easier for people who just want to focus on the code that produces certain visualizations without doing it in RStudio. Also, this get rid of most technical issues (hopefully) but the installation of packages will take a while and you will need to re-install the packages every time you want to re-use the Jupyter Notebook.

Choose which option suits you best and then go with it. 

## Timeline{-}

Here is what we have planned to cover in this workshop:

Friday, August 25, 10 am - 12 pm

* Introduction

* Primer

* Session preparation

* Principal Component Analysis

* Multidimensional Scaling 

* Factor Analysis

* Wrap-up

# What are dimension reduction methods? {-}

Dimension reduction methods such as Principal Component Analysis (PCA), Multidimensional Scaling (MDS), and Factor Analysis are techniques used to *reduce the number of variables or dimensions in a data set while preserving as much relevant information as possible*. 

The choice of method depends on the specific goals of the analysis and the nature of the data being analyzed. Each method addresses different aspects of dimension reduction:

+ PCA emphasizes variance

+ MDS emphasizes pairwise distances

+ Factor Analysis emphasizes the underlying latent factors.

Below are brief explanations of the three commonly used dimension reduction methods.

**Principal Component Analysis (PCA)**

Principal Component Analysis is a statistical technique that transforms a data set into a new coordinate system where the variables (features) are linearly uncorrelated.

It aims to find a set of orthogonal axes (principal components) in such a way that the first principal component accounts for the maximum variance in the data, the second principal component for the second maximum variance, and so on. By selecting a subset of these principal components, you can achieve dimension reduction while retaining the most significant information in the data.

**Multidimensional Scaling (MDS)**

Multidimensional Scaling is a method used to represent high-dimensional data in a lower-dimensional space (usually 2D or 3D) while maintaining pairwise distances or similarities between data points. MDS attempts to preserve the relationships among data points as much as possible. It's often used in visualization to represent complex data in a way that makes it easier to interpret or analyze.

**Factor Analysis**

Factor Analysis is a statistical technique that aims to uncover underlying latent factors that contribute to the observed variables in a data set. It's particularly useful when dealing with data where variables may be correlated, and you want to identify common factors that explain the shared variance. Factor Analysis assumes that observed variables are influenced by both the common factors and unique factors specific to each variable.

# Getting started {-}

If you choose option 2, you need to set up our R session and prepare our R project at the very beginning of the workshop. 

For everything to work, please do the following:

* Create a folder for this workshop somewhere on your computer, e.g. called *AcqVA_DimRed_WS*

* In that folder, create two subfolders called *data* and *images*

* Open RStudio, go to File > New Project > Existing Directory (Browse to project folder) > Create (and hit Enter)

This will then create an R Project in the project folder.

## Today's Data{-}


We will use 3 data sets:

+ **biopsy**: This breast cancer database was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. He assessed biopsies of breast tumours for 699 patients up to 15 July 1992; each of nine attributes has been scored on a scale of 1 to 10, and the outcome is also known. There are 699 rows and 11 columns.

+ **surveydata**: This fictitious data set represents responses to 15 items by 20 participants. The 15 items aim to assess 3 different psychological constructs (outgoingness, intelligence, and attidude).

```{r data1, message=F, warning=F}
data(biopsy)
report(biopsy)
```


# Principal Component Analysis{-}

Principal Component Analysis (PCA) is  used for dimensionality reduction and data compression while preserving as much variance as possible in the data. It achieves this by transforming the original data into a new coordinate system defined by a set of orthogonal axes called *principal components*. The first principal component captures the maximum variance in the data, the second captures the second maximum variance, and so on.

## Theoretical underpinnings of PCA explained

Data set.

```{r echo=F, message=F, warning=F}
pcadat <- data.frame(Language = c("Lang1", "Lang2", "Lang3", 
        "Lang4", "Lang5", "Lang6"),
        Feat1 = c(10, 11, 8, 3, 2, 1),
                     Feat2 = c(6, 4, 5, 3, 2.8, 1),
                     Feat3 = c(12, 9, 10, 2.5, 1.3, 2),
                     Feat4 = c(5, 7, 6, 2, 4, 7)) %>%
  as.data.frame() 
pcadat
```

**Primer: Why should I use a PCA and what does PCA tell us?** 

We start by plotting Feat1.

```{r echo = F, message=F, warning=F}
pcadat %>%
ggplot(aes(x = Feat1, y = rep(0, 6))) +
  geom_point(size = 5) +
  theme_bw() +
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  scale_x_discrete(name ="Feat1", limits=seq(1, 11, 1))+
  scale_y_discrete(name ="", limits=seq(-.5, .5, .5))
```

There appear to be two groups in our data!

We add Feat2.

```{r echo = F, message=F, warning=F}
pcadat %>%
  dplyr::mutate(col = ifelse(Feat1 > 5, "1", "0")) %>%
ggplot(aes(x = Feat1, y = Feat2)) +
  geom_point(size = 5) +
  theme_bw() +
  theme(legend.position = "none") +
  scale_x_discrete(name ="Feat1", limits=seq(1, 11, 1))+
  scale_y_discrete(name ="Feat2", limits=seq(1, 6, 1))
```

There still appear to be two groups (Feat2 not as distinctive).

We continue by adding Feat3 (as size).

```{r echo = F, message=F, warning=F}
pcadat %>%
ggplot(aes(x = Feat1, y = Feat2, group = Feat2, size = Feat3)) +
  geom_point() +
  theme_bw() +
  #theme(legend.position = "none") +
  scale_x_discrete(name ="Feat1", limits=seq(1, 11, 1))+
  scale_y_discrete(name ="Feat2", limits=seq(1, 6, 1))
```

Feat3 supports the two group hypothesis (small blow 5 big above 5).

We continue by adding Feat4 (as color).

```{r echo = F, message=F, warning=F}
pcadat %>%
ggplot(aes(x = Feat1, y = Feat2, size = Feat3, color = Feat4)) +
  geom_point() +
  theme_bw() +
  #theme(legend.position = "none") +
  scale_x_discrete(name ="Feat1", limits=seq(1, 11, 1))+
  scale_y_discrete(name ="Feat2", limits=seq(1, 6, 1))+
  scale_color_viridis_b()
```

Feat4 has not added much information.

> **PCA can tell us what feature(s) is/(are) responsible for the division into two groups**


Let's now go through a PCA step-by-step.

For this, we only look at Feat1 and Feat2.

```{r echo = F}
pcadat2 <- pcadat %>%
  dplyr::select(-Feat3, -Feat4)
# inspect
pcadat2
```

We start by calculating the center of the data.

```{r echo = F, message=F, warning=F}
pcadat %>%
ggplot(aes(x = Feat1, y = Feat2)) +
  geom_point(size = 5) +
  theme_bw() +
  theme(legend.position = "none") +
  scale_x_discrete(name ="Feat1", limits=seq(1, 11, 1))+
  scale_y_discrete(name ="Feat2", limits=seq(1, 11, 1)) +
  ggplot2::annotate(geom = "text", label = "+", x = mean(pcadat$Feat1), y = mean(pcadat$Feat2), size = 10)
```

Next we scale and center the data so that the center is at (0,0).


```{r echo = F, message=F, warning=F}
pcadat <- pcadat %>%
  dplyr::mutate(Feat1 = scale(Feat1),
                Feat2 = scale(Feat2))

pcadat %>%
ggplot(aes(x = Feat1, y = Feat2)) +
  geom_point(size = 5) +
  theme_bw() +
  theme(legend.position = "none") +
  ggplot2::annotate(geom = "text", label = "+", x = 0, y = 0, size = 10) +
  geom_vline(xintercept = 0, color = "red", linetype = "dotted") +
  geom_hline(yintercept = 0, color = "red", linetype = "dotted")
```

The relationship between data points is unchanged!

> Important: always center and scale your data as the scales will impact the magnitude of variance!
> In other words, if you do not center and scale, components can be deemed important simply because the scale of the variables is bigger (not because they are more important)!

Now, we fit a line through the data.

```{r echo = F, message=F, warning=F}
reg <- lm(formula = Feat2 ~ Feat1,
   data=pcadat) 

#get intercept and slope value
coeff<-coefficients(reg)          
intercept <- coeff[1]
slope <- coeff[2]

pcadat %>%
  ggplot(aes(x = Feat1, y = Feat2)) +
  geom_point(size = 5) +
  theme_bw() +
  theme(legend.position = "none") +
  ggplot2::annotate(geom = "text", label = "+", x = 0, y = 0, size = 10) +
  geom_vline(xintercept = 0, color = "red", linetype = "dotted") +
  geom_hline(yintercept = 0, color = "red", linetype = "dotted") +
  geom_abline(intercept = intercept, slope = slope, color="green", 
               linetype="dashed", size=1.5)
```

This line is called **Principal Component 1** (PC1).

PC1 has a slope of `r round(slope, 2)` - this already shows us that the data is spread much more along PC1 that along PC2!

We go about `round(1 / slope, 1)` units (1 / `r round(slope, 4)`) to the right and 1 unit up.

```{r echo = F, message=F, warning=F}
pcadat %>%
  ggplot(aes(x = Feat1, y = Feat2)) +
  geom_point(size = 5) +
  theme_bw() +
  theme(legend.position = "none") +
  ggplot2::annotate(geom = "text", label = "+", x = 0, y = 0, size = 10) +
  geom_vline(xintercept = 0, color = "red", linetype = "dotted") +
  geom_hline(yintercept = 0, color = "red", linetype = "dotted") +
  geom_abline(intercept = intercept, slope = slope, color="green", 
               linetype="dashed", size=1.5) +
  geom_segment(aes(x = 0, y = 0, xend = 1/slope, yend = 0),
                  arrow = arrow(length = unit(0.25, "cm")), color = "blue") +
  geom_segment(aes(x = 1/slope, y = 0, xend = 1/slope, yend = 1),
                  arrow = arrow(length = unit(0.25, "cm")), color = "blue") +
  geom_segment(aes(x = 0, y = 0, xend = 1/slope, yend = 1),
                  size = 1.5, arrow = arrow(length = unit(0.25, "cm")), color = "red") +
  ggplot2::annotate(geom = "text", label = "a", x = 0.6, y = 0.8, color = "red") +
  ggplot2::annotate(geom = "text", label = "b", x = 0.6, y = -0.25, color = "blue") +
  ggplot2::annotate(geom = "text", label = "c", x = 1.25, y = 0.5, color = "blue")
```

Now, we find the value for a (square root of c^2^ + b^2^) =  `r sqrt(1 + (1.187175 * 1.187175))` (1.552219). 

Next, we scale b and c so that a is 1.

a^2^ = b^2^ + c^2^

b = 1.187175 / 1.552219 = 0.7648244

c = 1 / 1.552219 = 0.644239


To make PC1, we need 0.765 of Feat1 and 0.644 of Feat2.

This 1 unit long vector (a) is called the Eigenvector for PC1.

> Eigenvector and Eigenvalue are not the same! 
> Eigenvector represent the factor loadings for each observed variable on a specific factor.
> Eigenvalue represents the amount of variance explained by an entire factor.

The proportions (0.765 and 0.644) are called the loading scores.

```{r echo = F, message=F, warning=F}
pcadat %>%
  ggplot(aes(x = Feat1, y = Feat2)) +
  geom_point(size = 5) +
  theme_bw() +
  theme(legend.position = "none") +
  ggplot2::annotate(geom = "text", label = "+", x = 0, y = 0, size = 10) +
  geom_vline(xintercept = 0, color = "red", linetype = "dotted") +
  geom_hline(yintercept = 0, color = "red", linetype = "dotted") +
  geom_abline(intercept = intercept, slope = slope, color="green", 
               linetype="dotted", size=1.5) +
  geom_segment(aes(x = 0, y = 0, xend = 0.7648244, yend = 0.644239), size = 1.5, 
                  arrow = arrow(length = unit(0.25, "cm")), color = "red") +
  ggplot2::annotate(geom = "text", label = "Eigenvector for the data point on PC1", x = 0.5, y = 0.8, color = "red")
```


PC2 is simply the perpendicular line to PC1 that goes through 0,0.

```{r echo = F, message=F, warning=F}
pcadat %>%
  ggplot(aes(x = Feat1, y = Feat2)) +
  geom_point(size = 5) +
  theme_bw() +
  theme(legend.position = "none") +
  ggplot2::annotate(geom = "text", label = "+", x = 0, y = 0, size = 10) +
  geom_vline(xintercept = 0, color = "red", linetype = "dotted") +
  geom_hline(yintercept = 0, color = "red", linetype = "dotted") +
  geom_abline(intercept = intercept, slope = slope, color="green", 
               linetype="dotted", size=1.5) +
    geom_abline(intercept = intercept, slope = -1/slope, color="blue", 
               linetype="dotted", size=1.5) +
  coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2))

```

As PC1 and PC2 are perpendicular, the Eigenvector of PC2 is -0.644 of Feat1 and 0.765 of Feat2.


```{r echo = F, message=F, warning=F}
angle_rad <- atan(slope)
angle_deg <- angle_rad * (180 / pi)
#angle_deg

degree <- 0-angle_deg  # Rotation angle in degrees
radians <- degree * (pi / 180)  # Convert degrees to radians

# Rotate the coordinates
pcadat_rotated <- data.frame(Feat1 = pcadat$Feat1 * cos(radians) - pcadat$Feat2 * sin(radians),
                             Feat2 = pcadat$Feat1 * sin(radians) + pcadat$Feat2 * cos(radians))

pcadat_rotated %>%
  ggplot(aes(x = Feat1, y = Feat2)) +
  geom_point(size = 5) +
  theme_bw() +
  theme(legend.position = "none") +
  geom_vline(xintercept = 0, color = "green", linetype = "dotted", size = 1.5) +
  geom_hline(yintercept = 0, color = "blue", linetype = "dotted", size = 1.5) +
  coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2))
```

We can (but have to) rotate the PC1 and make the points smaller (so that our plot looks more like a proper PCA plot).

```{r echo = F, message=F, warning=F}
# Rotate the coordinates
pcadat_rotated <- pcadat_rotated %>%
  dplyr::mutate(Feat1 = -Feat1) 

pcadat_rotated %>%
  ggplot(aes(x = Feat1, y = Feat2)) +
  geom_point() +
  theme_bw() +
  theme(legend.position = "none") +
  geom_vline(xintercept = 0, linetype = "dotted", size = 1) +
  geom_hline(yintercept = 0, linetype = "dotted", size = 1) +
  coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2)) +
  labs(x = "PC1", y = "PC2")
```

To plot the results of a PCA, we simply rotate the plot so that the PCs reflect the axes and we project the points onto the PCs.

```{r}
# generate data
df <- data.frame(pcadat$Feat1,
                 pcadat$Feat2)
colnames(df) <- c("Feat1", "Feat2")
# perform PCA
pcaex <- prcomp(df, center = T, scale = T)
# visualize PCA results
fviz_pca_ind(pcaex, label =  "") +
  coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2)) +
  ggtitle("PCA plot")
```


PCA effectively transforms your data from the original feature space to a lower-dimensional space defined by the principal components. The transformed data has reduced dimensions, with the first few dimensions capturing the most significant variation in the original data. This reduction is useful for visualization, noise reduction, and simplifying complex data sets. However, it's essential to note that interpretability of the principal components may not always be straightforward, as they are linear combinations of the original features.

## PCA in R 

### Example 1

Here, we will show

+ how to use the `prcomp()` function to perform PCA  

+ how to draw a PCA plot using ggplot2  

+ how to determine how much variation each component accounts for  

+ how to examine the loading scores to determine what variables have the largest effect on the graph

In this example, the data is in a matrix called `pcadat` where columns are individual samples (i.e. languages) and rows are measurements taken for all the samples (i.e. features).


```{r}
pcadat <- matrix(nrow=100, ncol=10)
colnames(pcadat) <- c(
  paste("indoeu", 1:5, sep=""), # Indo-European languages
  paste("austro", 1:5, sep="")) # Austronesian languages
rownames(pcadat) <- paste("feature", 1:100, sep="")
for (i in 1:100) {
  indoeu.values <- rpois(5, lambda=sample(x=10:1000, size=1))
  austro.values <- rpois(5, lambda=sample(x=10:1000, size=1))
 
  pcadat[i,] <- c(indoeu.values, austro.values)
}
# inspect data
head(pcadat); dim(pcadat)
```

Implement the PCA.

```{r}
pca <- prcomp(t(pcadat), scale=TRUE) 
```

Now, we generate a scree plot to show how much variance is accounted for by each component.

```{r}
# prepare data
pca.var <- pca$sdev^2
# extract percentage of contribution
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
# generate data frame for visualisation
pcascreedat <- data.frame(Component = paste0("PC", 1:10),
                          Percent = round(pca.var/sum(pca.var)*100, 2)) %>%
  dplyr::mutate(Text = paste0(Percent, "%"),
                Component = factor(Component, 
                                    levels = c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7", "PC8", "PC9", "PC10")))
# inspect
pcascreedat
```

Now, we generate the scree plot.

```{r}
ggplot(pcascreedat, aes(x = Component, y = Percent, label = Text)) +
  geom_bar(stat = "identity") +
  geom_text(vjust=-1.6, size = 3) +
  ggtitle("Scree Plot") +
  labs(x="Principal Components", y="Percent Variation") +
  theme_bw() +
  coord_cartesian(ylim = c(0, 100))
```

Now we make a fancy looking plot that shows the PCs and the variation.


```{r}
pca.data <- data.frame(Sample=rownames(pca$x),
                       X=pca$x[,1],
                       Y=pca$x[,2])
# inspect
pca.data
```

```{r} 
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
  geom_text() +
  xlab(paste("PC1 (", pca.var.per[1], "%)", sep="")) +
  ylab(paste("PC2 (", pca.var.per[2], "%)", sep="")) +
  theme_bw() +
  ggtitle("PCA Graph") +
  coord_cartesian(xlim = c(-11, 11))
```

Next, we get the name of the most importnat 5 features that contribute most to pc1.

```{r} 
loading_scores <- pca$rotation[,1]
# extract the magnitudes of the scores
feature_scores <- abs(loading_scores) 
# extract the 5 highest scores
feature_score_ranked <- sort(feature_scores, decreasing=TRUE)
top_5_features <- names(feature_score_ranked[1:5])
# show the names of the top 5 features 
top_5_features
```

Now, we show the scores (and +/- sign)

```{r} 
pca$rotation[top_10_features,1]
```
### Example 2

In this example, we use the `biopsy` data to see what variables can be used to predict malignant melanomas.

In a first step, we remove any data points with missing values.

```{r removeNA1, message=F, warning=F}
data(biopsy)
biopsy_nona <- biopsy %>%
  drop_na()
# inspect data
str(biopsy_nona)
```

Next, we remove non-numeric variables.

```{r reteinnum, message=F, warning=F}
biopsy_num <- biopsy_nona %>%
  dplyr::select_if(is.numeric)
# inspect data
summary(biopsy_num)
```

Once all variables are centered and scaled, we can generate a co-variance matrix (which is identical to a correlation matrix if variables are centered and scaled).

```{r}
biopsy_cor <- cor(biopsy_num)
# visualize
plot(biopsy_cor)
```


### Example 3

```{r}
surveydata <- base::readRDS(url("https://slcladal.github.io/data/sud.rda", "rb"))
# inspect
head(surveydata)
```


```{r}
# entering raw data and extracting PCs  from the correlation matrix
PrincipalComponents <- princomp(surveydata[c("Q01_Outgoing",    
                   "Q02_Outgoing",  
                   "Q03_Outgoing",  
                   "Q04_Outgoing",  
                   "Q05_Outgoing")], cor=TRUE)
summary(PrincipalComponents) # print variance accounted for
```

The cumulative proportion of variance already shows that the five items represent a single underlying factor as 91.5% of the variance is explained by PC1 alone!


```{r}
loadings(PrincipalComponents) # pc loadings
```

```{r}
plot(PrincipalComponents,type="lines") # scree plot
```


```{r}
PrincipalComponents$scores # the principal components
```

```{r}
pcsurveydat <- PrincipalComponents$scores %>%
  as.data.frame() %>%
  dplyr::rename(PC1 = 1,
                PC2 = 2,
                PC3 = 3,
                PC4 = 4,
                PC5 = 5)
# inspect
head(pcsurveydat)
```

```{r}
pcsurveydat %>%
  ggplot(aes(x = PC1, y = PC2)) +
  geom_point()
```



### Perform PCA



We can now perform the PCA.

```{r}
biopsy_pca <- prcomp(biopsy_num,
                     scale = T)
# summary
summary(biopsy_pca)
```

Check information of PCA object

```{r}
names(biopsy_pca)
```

Check the Eigenvectors which represent the loadings of the variables for each component.

```{r}
biopsy_pca$rotation
```

Check standard deviation and means of variables.

```{r}
biopsy_pca$center
biopsy_pca$scale
```

Extract PCA values for each data point.

```{r}
head(biopsy_pca$x, 10)
```


### Visualise results

First, we create a scree plot of the variance captured by each component.

```{r}
factoextra::fviz_eig(biopsy_pca,
                     addlabels = T,
                     ylim = c(0, 80))
```

Create biplot with default settings

```{r}
fviz_pca_biplot(biopsy_pca,
                # no data point labels
                label = "var")
```

Color by class.

```{r}
fviz_pca_biplot(biopsy_pca,
                # no data point labels
                label = "var",
                # color by class
                habillage = biopsy_nona$class)
```

Prettify the graph


```{r}
fviz_pca_biplot(biopsy_pca,
                # no data point labels
                label = "var",
                # color by class
                habillage = biopsy_nona$class,
                # change variable color 
                col.var = "black") + 
  # adapt data point color
                ggplot2::scale_color_manual(values = c("orange", "purple"))
```

# Multidimensional Scaling {-}

Multidimensional Scaling (MDS) is used for visualizing high-dimensional data in a lower-dimensional space while preserving pairwise distances or similarities between data points. MDS aims to represent the data in a way that maintains the relationships among data points as much as possible. 

There are two types of MDS:

+ classical MDS
+ 

MDS works exactly like PCA with one important difference: while PCA starts off with correlations between variables, MDS start of with distances!

So we re-use the data we used for PCA (Example 1).

As we already have the data, we can draw an MDS plot using the same data and the Euclidean distance. This graph should look the same as the PCA plot.

For MDS, we first calculate the distance matrix using the Euclidian distance. Note that we are transposing, scaling and centering the data just like for PCA.

```{r}
pcadat_dist <- dist(scale(t(pcadat), center=TRUE, scale=TRUE),
  method="euclidean")
```

Now, we do the MDS math (this is basically eigen value decomposition)

```{r}
mds.obj <- cmdscale(pcadat_dist, eig=TRUE, x.ret=TRUE)
```

Now, we calculate the percentage of variation that each MDS axis accounts for...

```{r}
mds.var.per <- round(mds.obj$eig/sum(mds.obj$eig)*100, 1)
# inspect
mds.var.per
```

We  make a fancy looking plot that shows the MDS axes and the variation:

```{r}
mds.values <- mds.obj$points
mds.data <- data.frame(Sample=rownames(mds.values),
  X=mds.values[,1],
  Y=mds.values[,2])
# inspect
mds.data
```



```{r}
ggplot(data=mds.data, aes(x=X, y=Y, label=Sample)) +
  geom_text() +
  theme_bw() +
  xlab(paste("MDS1 - ", mds.var.per[1], "%", sep="")) +
  ylab(paste("MDS2 - ", mds.var.per[2], "%", sep="")) +
  ggtitle("MDS plot using Euclidean distance")
```

Now draw an MDS plot using the same data and the average log(fold change). This graph should look different than the first two.

First, take the log2 of all the values in the data.matrix. This makes it easy to compute log2(Fold Change) between a gene in two samples since...

log2(Fold Change) = log2(value for sample 1) - log2(value for sample 2)

```{r} 
log2.pcadat <- log2(pcadat)
```

Now create an empty distance matrix

```{r}
log2.distance.matrix <- matrix(0,
  nrow=ncol(log2.pcadat),
  ncol=ncol(log2.pcadat),
  dimnames=list(colnames(log2.pcadat),
    colnames(log2.pcadat)))
# inspect 
log2.distance.matrix
```

now compute the distance matrix using avg(absolute value(log2(FC)))

```{r}
for(i in 1:ncol(log2.distance.matrix)) {
  for(j in 1:i) {
    log2.distance.matrix[i, j] <-
      mean(abs(log2.pcadat[,i] - log2.pcadat[,j]))
  }
}
# inspect
log2.distance.matrix
```

Now we do the MDS math (this is basically eigenvalue decomposition). The `cmdscale()` is the function for classical MDS.

```{r}
mds.stuff <- cmdscale(as.dist(log2.distance.matrix),
  eig=TRUE,
  x.ret=TRUE)
```

Calculate the percentage of variation that each MDS axis accounts for...

```{r}
mds.var.per <- round(mds.stuff$eig/sum(mds.stuff$eig)*100, 1)
# inspect
mds.var.per
```

Now we make a fancy looking plot that shows the MDS axes and the variation:


```{r}
mds.values <- mds.stuff$points
mds.data <- data.frame(Sample=rownames(mds.values),
  X=mds.values[,1],
  Y=mds.values[,2])
# inspect
mds.data
```


```{r}
ggplot(data=mds.data, aes(x=X, y=Y, label=Sample)) +
  geom_text() +
  theme_bw() +
  xlab(paste("MDS1 (", mds.var.per[1], "%)", sep="")) +
  ylab(paste("MDS2 (", mds.var.per[2], "%)", sep="")) +
  ggtitle("MDS plot using avg(logFC) as the distance") +
  coord_cartesian(xlim = c(-.7, .7))
```


# Factor Analysis {-}

Factor Analysis is used to identify underlying latent factors that explain the observed correlations among variables in a data set. It aims to reduce the complexity of high-dimensional data by identifying a smaller number of factors that contribute to the variance and covariance among variables. 

There are two types of factor analysis: 
+ exploratory factor analysis (EFA) 
+ confirmatory factor analysis (CFA)


Here, we are only going to look at EFA which is used to identify how many latent variables (factors) are present in a data set. In contrast to CFA, EFA allows factors to be correlated with each other and it uses a rotation step to transform the initial factor structure into a more interpretable form. Common rotation methods include *Varimax*, *Oblimin*, and *Promax*. 

CFA is used for hypothesis testing to confirm structures assumed to be present in the data. In R, we can use `lavaan` (Structural Equation Models) to implement CFA and test specific hypothesis about the existence and interactions of latent factors.  


### FA in R

We begin by loading data. This fictitious data set represents responses to 15 items by 20 participants. The 15 items aim to assess 3 different psychological constructs:

+ outgoingness (extroversion)
+ intelligence
+ attidude

```{r}
surveydata <- base::readRDS(url("https://slcladal.github.io/data/sud.rda", "rb"))
# inspect
report(surveydata)
```

For FA, we need to remove any nominal or categorical variables (Respondent).


```{r}
surveydata <- surveydata %>% 
  dplyr::select(-Respondent)
# inspect
str(surveydata)
```

We can now implement the EFA using the `factanal` function from the base `stats` package. Although `factanal` implements a EFA, we need to provide it with the number of factors (latent variables) it should look for (commonly, you need to vary this to find the optimal number of factors).



```{r}
factoranalysis <- factanal(surveydata, # data
                           3) # number of factors
print(factoranalysis, 
      digits = 2,  # round to x decimals
      cutoff = .2, # do not show loadings smaller than .2
      sort = TRUE) # show items sorted
```

Let us examine the output.

We start with the *function call*, followed by *uniqueness* scores. Uniqueness refers to the portion of the variance in an observed variable that is not explained by the underlying factors extracted from the data. It represents the variability in a variable that is unique to that variable and cannot be accounted for by the common factors identified in the analysis. This means that items that do not capture or represent a latent variable well, will have high uniqueness scores (see, e.g., *Q10_Intelligence*).

Next, the output shows a table with factor loadings. *Loadings* are correlations between an observed variable and a latent factor. Loadings indicate how strongly each observed variable is associated with each extracted factor. They provide insights into which factors influence the variation in each observed variable. 

We then get a table showing the sum-of-squares loadings with the amount of variance explained by each latent variable which can help identify the optimal or necessary number of latent variables (factors) that we should or need to look for. 

The p-value should not be significant (if it is significant, then you have missed factors). In our case, even 2 factors would report a non significant result due to the low number of data points (but p is highest for 3 factors).

We can now plot the results to see if and how the different items group together. We start by created a data frame from the FA results containing the information we need for the visualization.

```{r}
fadat <- data.frame(Factor1 = factoranalysis$loadings[,1],
                    Factor2 = factoranalysis$loadings[,2],
                    Items = names(surveydata))
# inspect
head(fadat)
```

Now, that the data is formatted appropriately, we can plot it.

```{r}
fadat %>%
  ggplot(aes(x = Factor1, y = Factor2, label = Items, color = Factor1)) +
  geom_text() + 
  theme_bw() +
  coord_cartesian(xlim = c(-1.5, 1.5), ylim = c(-1, 1.5)) +
  scale_color_viridis_b() +
  labs(x = "Factor 1 (49 %)", y = "Factor 2 (32 %)")
```

As we can see, there are 3 groups (factors) with *Q10_Intelligence* not aligning well with the other elements in that group. This could suggest that *Q10_Intelligence* is not an optimal item / variable for reflecting (or capturing) the latent intelligence factor.

# Wrap-up{-}

That's all folks!

# Citation & Session Info {-}

Schweinberger, Martin. `r format(Sys.time(), '%Y')`. *Introduction to Dimension Reduction Methods with R - AcqVA Aurora workshop*. Tromsø: The Arctic University of Norway, Tromsø. url: https://slcladal.github.io/introviz.html  (Version `r format(Sys.time(), '%Y.%m.%d')`).

```
@manual{schweinberger`r format(Sys.time(), '%Y')`acqvavizrws,
  author = {Schweinberger, Martin},
  title = {Introduction to Dimension Reduction Methods with R - AcqVA Aurora workshop},
  note = {https://slcladal.github.io/introviz.html},
  year = {`r format(Sys.time(), '%Y')`},
  organization = "The Arctic University of Norway (UiT), AcqVA Aurora Center},
  address = {Tromsø},
  edition = {`r format(Sys.time(), '%Y.%m.%d')`}
}
```


```{r fin}
sessionInfo()
```

***

[Back to top](#introduction)


***

# References {-}


